services:
# docker compose -f compose.yml --profile pack up --build 

# Embeddings generation:
# docker compose -f compose.yml --profile generation up --build 

# Start the application:
# docker compose -f compose.yml --profile application up --build 

  elasticsearch:
    profiles: [generation, injection, application]
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2
    container_name: elasticsearch_ama
    ports:
      - 9200:9200
    environment:
      - node.name=elasticsearch
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - xpack.license.self_generated.type=trial
      - ES_JAVA_OPTS=-Xmx8g
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=500ms"]
      retries: 300
      interval: 1s

  elasticsearch_settings:
    profiles: [generation, injection, application]
    depends_on:
      elasticsearch:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2
    container_name: elasticsearch_settings_ama
    restart: 'no'
    command: >
      bash -c '        
        echo "Setup the kibana_system password";
        until curl -s -u "elastic:${ELASTIC_PASSWORD}" -X POST http://elasticsearch:9200/_security/user/kibana_system/_password -d "{\"password\":\"${ELASTIC_PASSWORD}\"}" -H "Content-Type: application/json" | grep -q "^{}"; do sleep 5; done;
      '

  kibana:
    profiles: [generation, injection, application]
    image: docker.elastic.co/kibana/kibana:8.17.2
    container_name: kibana_ama
    depends_on:
      elasticsearch_settings:
        condition: service_completed_successfully
    ports:
      - 5601:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=${ELASTIC_HOSTS}
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'All services are available'"]
      retries: 300
      interval: 1s

  download-local-llm-embeddings:
    profiles: [generation, injection, application]
    image: curlimages/curl:8.12.1
    #entrypoint: ["curl", "http://host.docker.internal:11434/api/pull", "-d", '{"name": "nomic-embed-text:latest"}']
    entrypoint:
      - "sh"
      - "-c"
      - 'curl "${OLLAMA_BASE_URL}/api/pull" -d "{\"name\": \"${LLM_EMBEDDINGS}\"}"'
    extra_hosts:
        - "host.docker.internal:host-gateway"


  pack-files:
    profiles: [generation, pack]
    build:
      context: ./generate-content
      dockerfile: Dockerfile
    volumes:
      # Mount your input directory here
      - ./input:/data/input
      # Mount a directory for output files
      - ./data:/data/output
    environment:
      # Set environment variables (override defaults from Dockerfile if needed)
      - INPUT_DIR=${INPUT_DIR}
      - INCLUDE_EXTS=${INCLUDE_EXTS}
      - EXCLUDE_EXTS=${EXCLUDE_EXTS}
      - STRUCTURE_FILE=${STRUCTURE_FILE}
      - CONTENT_FILE=${CONTENT_FILE}
      - SUMMARY_FILE=${SUMMARY_FILE}

  create-embeddings:
    profiles: [generation]
    build:
      context: ./create-embeddings
      dockerfile: Dockerfile    
    environment:
      - CONTENT_PATH=/app/data/content.txt
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
      - ELASTICSEARCH_HOSTS=${ELASTIC_HOSTS}
      - ELASTICSEARCH_USERNAME=${ELASTIC_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_INDEX=${ELASTIC_INDEX}
      #- LANGUAGE=${LANGUAGE}
      - CONTENT_PREFIX_FOR_ID=vectors
    volumes:
      - ./data:/app/data
    depends_on:
      pack-files:
        condition: service_completed_successfully
      download-local-llm-embeddings:
        condition: service_completed_successfully
      kibana:
        condition: service_started
      elasticsearch:
        condition: service_started
    extra_hosts:
        - "host.docker.internal:host-gateway"

# docker compose -f compose.yml --profile injection up --build 
# docker compose -f compose.yml up inject-content 
  inject-content:
    profiles: [generation, injection]
    build:
      context: ./create-embeddings
      dockerfile: Dockerfile    
    environment:
      - CONTENT_PATH=/app/content-to-inject/content.md
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
      - ELASTICSEARCH_HOSTS=${ELASTIC_HOSTS}
      - ELASTICSEARCH_USERNAME=${ELASTIC_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_INDEX=${ELASTIC_INDEX}
      - LANGUAGE=${LANGUAGE}
      - CONTENT_PREFIX_FOR_ID=injections
    volumes:
      - ./content-to-inject:/app/content-to-inject
    depends_on:
      download-local-llm-embeddings:
        condition: service_completed_successfully
      kibana:
        condition: service_started
      elasticsearch:
        condition: service_started
    extra_hosts:
        - "host.docker.internal:host-gateway"

  download-local-llm:
    profiles: [application]
    image: curlimages/curl:8.12.1
    #entrypoint: ["curl", "http://host.docker.internal:11434/api/pull", "-d", '{"name": "qwen2.5:7b"}']
    entrypoint:
      - "sh"
      - "-c"
      - 'curl "${OLLAMA_BASE_URL}/api/pull" -d "{\"name\": \"${LLM_CHAT}\"}"'
    extra_hosts:
        - "host.docker.internal:host-gateway"


  backend:
    profiles: [application]
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      - DIRECTORY_TREE_PATH=/app/data/tree.txt
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM_CHAT=${LLM_CHAT}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
      - ELASTICSEARCH_HOSTS=${ELASTIC_HOSTS}
      - ELASTICSEARCH_USERNAME=${ELASTIC_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_INDEX=${ELASTIC_INDEX}
      - MAX_SIMILARITIES=${MAX_SIMILARITIES}
      - HISTORY_MESSAGES=${HISTORY_MESSAGES}
      - OPTION_TEMPERATURE=${OPTION_TEMPERATURE}
      - OPTION_REPEAT_LAST_N=${OPTION_REPEAT_LAST_N}
      - OPTION_REPEAT_PENALTY=${OPTION_REPEAT_PENALTY}
      - OPTION_TOP_P=${OPTION_TOP_P}
      - OPTION_TOP_K=${OPTION_TOP_K}
      - OPTION_NUM_CTX=${OPTION_NUM_CTX}
      - SYSTEM_INSTRUCTIONS_PATH=${SYSTEM_INSTRUCTIONS_PATH}

    volumes:
      - ./data:/app/data
      - ./instructions:/app/instructions
    depends_on:
      download-local-llm-embeddings:
        condition: service_completed_successfully
      download-local-llm:
        condition: service_completed_successfully
      kibana:
        condition: service_started
      elasticsearch:
        condition: service_started
    develop:
      watch:
        - action: rebuild
          path: ./backend/server.js
    extra_hosts:
      - "host.docker.internal:host-gateway"




  frontend:
    profiles: [application]
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - 9090:8501
    environment:
      - BACKEND_SERVICE_URL=http://backend:5050
      - PAGE_TITLE=ðŸ¤“ AMA about your source code!
      - PAGE_HEADER=Made with ðŸ’– and probably too much caffeine
      - PAGE_ICON=ðŸ¤–
      - LLM_CHAT=${LLM_CHAT}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
    depends_on:
      - backend
    develop:
      watch:
        - action: rebuild
          path: ./frontend/app.py




